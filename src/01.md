# Week 1 — Intelligent Agents #

## Table of Contents ##

- [Week 1 — Intelligent Agents](#week-1--intelligent-agents)
  - [Table of Contents](#table-of-contents)
  - [Agents and Environments](#agents-and-environments)
  - [Rationality](#rationality)
    - [Rational Agents](#rational-agents)
    - [Information Gathering](#information-gathering)
  - [Characteristics of Environments](#characteristics-of-environments)
    - [Fully observable (vs. partially observable)](#fully-observable-vs-partially-observable)
    - [Single-agent (vs. multi-agent)](#single-agent-vs-multi-agent)
    - [Deterministic (vs. stochastic)](#deterministic-vs-stochastic)
    - [Episodic (vs. sequential)](#episodic-vs-sequential)
    - [Dynamic (vs. static)](#dynamic-vs-static)
    - [Discrete (vs. continuous)](#discrete-vs-continuous)
  - [Agent Programs](#agent-programs)
    - [Table-Lookup Agents](#table-lookup-agents)
    - [Simple Reflex Agents](#simple-reflex-agents)
    - [Model-based Reflex Agents](#model-based-reflex-agents)
    - [Goal-based Agents](#goal-based-agents)
    - [Utility-based Agents](#utility-based-agents)
    - [Learning Agents](#learning-agents)
      - [An Aside: Exploitation vs Exploration](#an-aside-exploitation-vs-exploration)

## Agents and Environments ##

An **agent** is anything that can be viewed as perceiving its **environment** through **sensors** and acting upon that environment with **actuators**. For example, humans have eyes and other organs for sensors, as well as hands, legs, and various other body parts for actuators.

**Percepts** are the content that an agent's sensors perceive, and the complete history of percepts is the agent's **percept sequence**.

> In general, an agent's choice of action at any given instant depends on its built-in knowledge and on the entire percept sequence, but not anything it has yet to perceive.

## Rationality ##

In order for an agent to know to do the right thing (through its sequence of actions), we need a measure of **desirability** for the outcome sequence of states in the environment. This measure is called a **performance measure**. It encompasses several metrics:

1. Whom is it best for?
2. What are we optimizing for?
3. What information is available?
4. What are the unintended effects?
5. What are the costs?

> As a general rule, it is better to design performance measures according to what one actually wants to be achieved in the environment rather than according to how one thinks the agent should behave.

### Rational Agents ###

For each possible percept sequence, a **rational agent** will choose an action that is **expected to maximize its performance measure**, given the evidence provided by the percept sequence and its built-in knowledge.

It is important to distinguish between rationality and **omniscience**. Rationality maximizes **expected performance**, while perfection maximizes **actual performance**. Rationality does not require omniscience, since rational choice depends only on the percept sequence *to date*.

Therefore, `rationality != omniscience`!

### Information Gathering ###

Agents can perform actions to **modify future percepts** to gather useful information, from which they can **learn**. An agent with he ability to adapt and modify its behavior is said to be **autonomous**. For example, a vacuum-cleaning agent that learns to predict where and when dirt appears will do better than one that does not.

## Characteristics of Environments ##

**Task environments** are the "problems" to which rational agents are "solutions". **PEAS** is used to define them, and it is an acronym for the following (with examples for autonomous driving):

- **P**erformance measure (safety, speed, comfort)
- **E**nvironment (roads, weather, visibility, other vehicles, pedestrians)
- **A**ctuators (Steering wheel, accelerator, brakes, signals)
- **S**ensors (cameras, LIDAR, speedometer, GPS)

Categorizing task environments by a small number of characteristics enables appropriate agent design.

### Fully observable (vs. partially observable) ###

A **fully observable environment** is one in which **an agent's sensors give it access to its complete state at each point in time**. Environments may be **partially observable** because of noisy or inaccurate sensors - or essentially if any sensor data is missing. If the agent lacks sensors, then the environment is **unobservable**, but the agent is not entirely hopeless.

### Single-agent (vs. multi-agent) ###

An agent in a **single-agent environment** operates by itself and does not consider other objects as agents. **Multi-agent environments** may be competitive or co-operative.

### Deterministic (vs. stochastic) ###

A **deterministic environment** allows its next state to be **completely determined by the current state and the action executed by the agent**. Otherwise, it is **stochastic**. In multi-agent environments, if the environment is deterministic except for the actions of other agents, then it is **strategic**.

### Episodic (vs. sequential) ###

In an **episodic task environment**, **the agent's experience is divided into atomic episodes**, during which the agent receives a percept and performs a single action. **The choice of action in each episode depends only on the episode itself.** Otherwise, the environment is **sequential**.

### Dynamic (vs. static) ###

**If the environment can change while an agent is deliberating, it is dynamic.** Otherwise, it is **static**. In a dynamic environment, when an agent has not decided on its action when asked, **it counts as deciding to do nothing**.
If the environment itself does not change with time but the agent's performance score does, the environment is **semi-dynamic** — an example of which is chess played with a clock.

### Discrete (vs. continuous) ###

A **discrete environment** has a finite number of distinct and clearly-defined percepts and actions, while **continuous environments** do not.

## Agent Programs ##

An agent's behavior is described by the **agent function** `[f: P* -> A]` that maps from percept histories to actions. This agent function is implemented internally in an artificial agent by an **agent program** which runs on physical **agent architecture** to produce `f`.

Summarily, `agent = architecture + program`, and the goal is to implement the rational agent function concisely.

### Table-Lookup Agents ###

```python
def table_lookup_agent_program(table):
    percepts = []

    def program(percept):
        percepts.append(percept)
        action = table.get(tuple(percepts))
        return action

    return program
```

A **table-lookup agent** keeps track of the percept sequence, using it to index into a table of actions to decide what to do. However, it suffers from several drawbacks:

- Storage space (expoential growth with respect to number of percepts)
- Lack of autonomy
- Duration to compute table entries

Nonetheless, table-lookup agents does do what we want, assuming the table is filled in correctly, since it implements the rational agent function.

### Simple Reflex Agents ###

```python
def simple_reflex_agent_program(rules, interpret_input):

    def program(percept):
        state = interpret_input(percept)
        rule = rule_match(state, rules)
        action = rule.action
        return action

    return program
```

The simplest kind of agents, **simple reflex agents** rely on their sensors to **perceive their environment in its current state, ignoring their percept history**. Then, **condition-action rules** govern the action which they perform.

However, simple reflex agents perform best in fully observable environments. They are vulnerable to infinite loops when their environment is only partially observable.

### Model-based Reflex Agents ###

```python
def model_based_reflex_agent_program(rules, update_state, model):

    def program(percept):
        program.state = update_state(program.state, program.action, percept, model)
        rule = rule_match(program.state, rules)
        action = rule.action
        return action

    program.state = program.action = None
    return program
```

The most effective way for an agent to handle partially observability, is to **keep track of the part of the environment it cannot currently see** in some form **internal state**. The information in this internal state has to be updated over time.

First, a **transition model** is needed for the agent to understand the effects of its actions, as well as how the world evolves independently of the agent.

Second, a **sensor model** represents the knowledge of how the state of the world is reflected in the agent's percepts.

Together, the use of such models enables **model-based reflex agents**.

### Goal-based Agents ###

Knowing the current state of the environment is not always sufficient for a decision. Agents may require **goal information** that describes desirable outcomes, such that the agent can choose the actions to achieve the goal.

### Utility-based Agents ###

Goals alone are usually insufficient to generate high-quality behavior in most environments, since goals dichotomize states into favorable and disfavorable states. Rather, the use of **utility functions** by agents allows the **internalization of the performance measure**.

The utility function measures the agent's preferences among states of the world, allowing the agent to choose the action that leads to the best **expected utility**.

### Learning Agents ###

Learning allows agents to operate in initially unknown environments and to become more competent that allowed by its initial knowledge alone.

A learning agent has four conceptual components:

1. The **learning element**, which makes improvements.
2. The **performance element**, which selects external actions.
3. The **critic**, which provides feedback on how the agent is doing with respect to a fixed performance standard. It also determines how the performance element should be modified to do better in future.
4. The **problem generator**, which suggests actions for new and informative experiences, especially through **exploration**.

#### An Aside: Exploitation vs Exploration ####

Agents in the real world must often choose between **maximizing expected utility with its current knowledge of the world (exploitation)** and **learning more about the world (exploration)**.

> The performance element of a learning agent would prefer to keep doing actions that are best, given what it knows. **However, if the agent is willing to explore a little and perhaps do some sub-optimal actions in the short run, it might discover much better actions for the long run.** The problem generator's role is also to suggest these exploratory actions.

This problem is known as the trade-off between exploitation and exploration.
