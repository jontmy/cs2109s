\section{Informed Search}
    Informed search involves the use an \textbf{evaluation function} $f(n)$ which estimates desirability for every node in the search tree.

    These search algorithms can therefore \textbf{find solutions more efficiently} by expanding the most desirable unexpanded nodes first.

    A \textbf{heuristic function} $h(n)$ estimates the cost from state $n$ to a goal state. Heuristics form a component of the evaluation function.

    \subsection{Greedy Best-first Search}
        Greedy best first search is a special case of best-first search which always \textbf{expands the node that appears closest to the goal} --- the node with the lowest $h(n)$ value.

        This algorithm is usually implemented with a \textbf{priority queue}, with nodes in the frontier \textbf{ordered in decreasing order of desirability}. The evaluation function $f(n) = h(n)$.

        Greedy best-first search is \textbf{not complete}, as it can get stuck in loops.

        It is \textbf{not optimal} either, since it tries to get as close to a goal as possible at each iteration, but this greediness can lead to a solution with a higher cost. \emph{In general, greedy algorithms are not optimal.}

        It has a \textbf{time complexity of} \bm{$O(b^m)$}, but a good heuristic function can reduce this significantly to $O(bm)$.

        Likewise, it has a \textbf{space complexity of} \bm{$O(b^m)$} since it needs to store every node in memory.

    \subsection{A* Search}
        A* search builds on best-first search by \textbf{avoiding the expansion of expensive nodes}, using an evaluation function $f(n) = g(n) + h(n)$.

        $f(n)$ estimates the total cost of the path from $n$ to a goal. $g(n)$ is the total cost to reach $n$ from the initial state, while $h(n)$ \textbf{estimates} the cost from $n$ to a goal.

        A* search is \textbf{complete} as long as the state space is finite.

        It is \textbf{optimal} as long as its heuristic is \textbf{admissible}. \emph{An admissible heuristic never overestimates the cost to reach a goal, and is therefore optimistic.}

        Formally, a heuristic is admissible if $\forall n(h(n) \le h^*(n))$, where $h^*(n)$ is the true cost to reach a goal state from node $n$.

        \textbf{If \bm{$h(n)$} is admissible, \code{TREE-SEARCH} is optimal.}

        Furthermore, a heuristic is consistent if, for every node $n$ and every successor $n'$ generated by an action $a$, $h(n) \le \text{cost}(n, a, n') + h(n')$.

        \textbf{If \bm{$h(n)$} is consistent, \code{GRAPH-SEARCH} is optimal.}

        \emph{Every consistent heuristic is admissible, but not vice versa.} Therefore, with a consistent heuristic, A* search is optimal as well.

        A* search has a \textbf{time complexity of} \bm{$O(b^m)$}. However, A* search can \textbf{prune} away nodes not required for finding an optimal solution, \emph{such that it is much more efficient than uninformed searches despite its exponential complexity}.

        It also has a \textbf{space complexity of} \bm{$O(b^m)$}, since every node needs to be stored in memory.

    \subsection{Memory-bounded Heuristic Search}
        With memory being the main issue with A* search, other algorithms exist with tricks to reduce the space complexity.

        \subsubsection{Iterative-deepening A* Search}
        IDA* search is to A* what iterative-deepening search is to depth-first search.

        Instead of setting the cutoff based on depth, IDA* uses the $f$-cost of $g(n) + h(n)$, incrementing the cutoff to the best $f$-cost which exceeded the cutoff on the previous iteration.

        \subsubsection{Recursive Best-first Search}
        RBFS resembles the typical best-first search, but keeps track of the best alternative $f$-value before exploring the current node.

        If the current node exceeds this limit, it recursively backtracks to explore the alternative path. While backtracking, it replaces the $f$-value of nodes along the way with the best $f$-value of that node's children.

        \subsubsection{Simplified Memory-bounded A* Search}
        A* search uses too much memory, while both IDA* and RBFS use too little memory --- IDA* keeps only the $f$-cost limit, while RBFS uses linear space, which under-utilizes memory even if more is available.

        SMA* improves on this by expanding the best node until all memory is used up.

        Then, it drops the node with the highest $f$-value, backtracking and replacing that node's parent's $f$-value with the dropped node's $f$-value before continuing.

        This algorithm is not perfect, however. On very hard problems, SMA* can switch back and forth between alternative paths, each time running out of memory and repeatedly regenerating the same nodes.

    \subsection{Formulating Heuristics}
        \emph{A search algorithm using a more efficient heuristic will never expand more nodes than one with a less efficient heuristic.}

        Given two admissible heuristics, $h_1(n)$ and $h_2(n)$, $h_2$ \textbf{dominates} $h_1$, if for all nodes $n$, $h_2(n) \ge h_1(n)$.

        Therefore, it is better to use a heuristic with higher $h(n)$ values, since it will expand fewer nodes as long as it is consistent.

        A \textbf{relaxed problem} is a modified problem with fewer restrictions placed on its actions. Any solution for the orgiginal problem also solves the relaxed problem.

        \emph{The cost of an optimal solution to a relaxed problem is an admissible and consistent heuristic for the original problem.}

    \subsection{Using Inadmissible Heuristics}
        If we are willing to to accept suboptimal solutions in exchange for fewer nodes explored, \textbf{inadmissible heuristics} can be used.

        Inadmissible heuristics may overestimate costs, risk missing the optimal solution, but if they are more accurate, they can lead to fewer nodes explored.

        This is especially useful when the state-space is large, and optimality is not a requirement.

\section{Local Search}
    \textbf{Local search} algorithms search unsystematically from an initial state to its neighbors \emph{without keeping track of paths and previous states}.

    They are useful in \textbf{optimization problems} where finding the best possible state is important, and the path to get there is irrevelant.

    \subsection{Hill-climbing Search}
        On each iteration, \textbf{hill-climbing search} keeps track of one current state, and moves to the neighboring state with the \textbf{steepest ascent}.

        It does not look beyond neighboring states, terminating at a peak, which means it can get stuck on \textbf{local maxima} and \textbf{plateaus}.

        A solution to plateaus is allowing \textbf{sideways movement}, while also restricting the number of these moves so that the algorithm does not wander the plateau forever.

        When there are many successors to any given state, choosing the \textbf{first-better-choice} is preferable. Another variant is \textbf{stochastic hill climbing}, which chooses a better successor randomly.

    \subsection{Simulated Annealing}
        Hill-climbing algorithms get stuck on \textbf{local maxima} since they never make downhill moves, while \textbf{random walk} algorithms will eventually reach the global maxima, albeit inefficiently.

        By combining the two, doing something random once in a while allows for breaking out of local maxima, \emph{while yielding both efficiency and completeness}.

        Simulated annealing algorithms pick a random move each iteration, accepting it if it improves the situation. Otherwise, it is accepted with a probability inversely and exponentially proportional to its "badness".

        Also, simulated annealing algorithms become less likely to accept bad moves as time goes on. \emph{With enough time, it will find a global maxima with probability approaching 1}.

    \subsection{Beam Search}
        Rather than just keeping one node in memory, \textbf{beam search} performs $k$ hill-climbing searches in parallel.

        In \textbf{local beam search}, information can be shared between parallel search threads, allowing the algorithm to abandon unfruitful searches.

        However, this means that all $k$ states may end up clustered together in the state space, effectively making it a $k$-times slower hill-climbing search. To combat this, \textbf{stochastic beam search} randomly chooses successors independently instead.

    \subsection{Genetic Algorithms}
        In \textbf{genetic algorithms}, successor states are generated from a \textbf{recombination} of two parent states at random \textbf{crossover points}, and \textbf{random mutations} are allowed.

        Not all successors are kept --- \textbf{selection processes} evaluate individuals with a \textbf{fitness function}, \textbf{culling} those that do not meet a fitness threshold.

\subsection{Online Search}
    So far, the algorithms have all been for \textbf{offline search}, in which the algorithms compute a complete solution and then execute it.

    \textbf{Online search}, however, interleaves the computation and execution. It is useful in dynamic environments where the agent is penalized for extended computation.

    It is also necessary in \textbf{exploration problems}, where the agent is clueless about the states and actions of its environment, and new observations are available only after acting.