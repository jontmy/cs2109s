\begin{multicols*}{2}
    \section{Informed Search}
        Informed search involves the use an \textbf{evaluation function} $f(n)$ which estimates desirability for every node in the search tree.

        These search algorithms can therefore \textbf{find solutions more efficiently} by expanding the most desirable unexpanded nodes first.

        A \textbf{heuristic function} $h(n)$ estimates the cost from state $n$ to a goal state. Heuristics form a component of the evaluation function.

        \subsection{Greedy Best-first Search}
            Greedy best first search is a special case of best-first search which always \textbf{expands the node that appears closest to the goal} --- the node with the lowest $h(n)$ value.

            This algorithm is usually implemented with a \textbf{priority queue}, with nodes in the frontier \textbf{ordered in decreasing order of desirability}. The evaluation function $f(n) = h(n)$.

            Greedy best-first search is \textbf{not complete}, as it can get stuck in loops.

            It is \textbf{not optimal} either, since it tries to get as close to a goal as possible at each iteration, but this greediness can lead to a solution with a higher cost. \emph{In general, greedy algorithms are not optimal.}

            It has a \textbf{time complexity of} \bm{$O(b^m)$}, but a good heuristic function can reduce this significantly to $O(bm)$.

            Likewise, it has a \textbf{space complexity of} \bm{$O(b^m)$} since it needs to store every node in memory.

        \subsection{A* Search}
            A* search builds on best-first search by \textbf{avoiding the expansion of expensive nodes}, using an evaluation function $f(n) = g(n) + h(n)$.

            $f(n)$ estimates the total cost of the path from $n$ to a goal. $g(n)$ is the total cost to reach $n$ from the initial state, while $h(n)$ \textbf{estimates} the cost from $n$ to a goal.

            A* search is \textbf{complete} as long as the state space is finite.

            It is \textbf{optimal} as long as its heuristic is \textbf{admissible}. \emph{An admissible heuristic never overestimates the cost to reach a goal, and is therefore optimistic.}

            Formally, a heuristic is admissible if $\forall n(h(n) \le h^*(n))$, where $h^*(n)$ is the true cost to reach a goal state from node $n$.

            \textbf{If \bm{$h(n)$} is admissible, \code{TREE-SEARCH} is optimal.}

            Furthermore, a heuristic is consistent if, for every node $n$ and every successor $n'$ generated by an action $a$, $h(n) \le \text{cost}(n, a, n') + h(n')$.

            \textbf{If \bm{$h(n)$} is consistent, \code{GRAPH-SEARCH} is optimal.}

            \emph{Every consistent heuristic is admissible, but not vice versa.} Therefore, with a consistent heuristic, A* search is optimal as well.

            A* search has a \textbf{time complexity of} \bm{$O(b^m)$}. However, A* search can \textbf{prune} away nodes not required for finding an optimal solution, \emph{such that it is much more efficient than uninformed searches despite its exponential complexity}.

            It also has a \textbf{space complexity of} \bm{$O(b^m)$}, since every node needs to be stored in memory.

        \subsection{Memory-bounded Heuristic Search}
            With memory being the main issue with A* search, other algorithms exist with tricks to reduce the space complexity.

            \subsubsection{Iterative-deepening A* Search}
            IDA* search is to A* what iterative-deepening search is to depth-first search.

            Instead of setting the cutoff based on depth, IDA* uses the $f$-cost of $g(n) + h(n)$, incrementing the cutoff to the best $f$-cost which exceeded the cutoff on the previous iteration.

            \subsubsection{Recursive Best-first Search}
            RBFS resembles the typical best-first search, but keeps track of the best alternative $f$-value before exploring the current node.

            If the current node exceeds this limit, it recursively backtracks to explore the alternative path. While backtracking, it replaces the $f$-value of nodes along the way with the best $f$-value of that node's children.

            \subsubsection{Simplified Memory-bounded A* Search}
            A* search uses too much memory, while both IDA* and RBFS use too little memory --- IDA* keeps only the $f$-cost limit, while RBFS uses linear space, which under-utilizes memory even if more is available.

            SMA* improves on this by expanding the best node until all memory is used up.

            Then, it drops the node with the highest $f$-value, backtracking and replacing that node's parent's $f$-value with the dropped node's $f$-value before continuing.

            This algorithm is not perfect, however. On very hard problems, SMA* can switch back and forth between alternative paths, each time running out of memory and repeatedly regenerating the same nodes.

        \subsection{Formulating Heuristics}
            \emph{A search algorithm using a more efficient heuristic will never expand more nodes than with a less efficient heuristic.}

            Given two admissible heuristics, $h_1(n)$ and $h_2(n)$, $h_2$ \textbf{dominates} $h_1$, if for any node $n$, $h_2(n) \ge h_1(n)$.

            Therefore, it is better to use a heuristic with higher $h(n)$ values, since it will expand fewer nodes as long as it is consistent.

            A \textbf{relaxed problem} is a modified problem with fewer restrictions placed on its actions. Any solution for the orgiginal problem also solves the relaxed problem.

            \emph{The cost of an optimal solution to a relaxed problem is an admissible and consistent heuristic for the original problem.}

        \subsection{Using Inadmissible Heuristics}
            If we are willing to to accept suboptimal solutions in exchange for fewer nodes explored, \textbf{inadmissible heuristics} can be used.

            Inadmissible heuristics may overestimate costs, risk missing the optimal solution, but if they are more accurate, they can lead to fewer nodes explored.

            This is especially useful when the state-space is large, and optimality is not a requirement.

\end{multicols*}